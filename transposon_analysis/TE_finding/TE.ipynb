{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#README\n",
    "\n",
    "# TE discovery in a genome assembly\n",
    "\n",
    "## Overview\n",
    "\n",
    "These are a set of wrappers compossing a pipline which finds transposable elements in a genome assembly. The pipline includes the following steps:  \n",
    "\n",
    "1. **MAKE A DENOVO LIB WITH REPEAT-MODELER**. **Input:** genome assembly, **Output:** a library containing partially classified consensus sequences of *de-novo* repeat clusters. **Program:** [RepeatModeler](http://www.repeatmasker.org/RepeatModeler.html) and all its many dependencies.\n",
    "\n",
    "2. **ADD CLASSIFICATIONS FROM THE ONLINE [CENSOR](http://www.girinst.org/censor/) TEXT OUTPUT TO THE REPEAT LIB**. **Input:** A) a library containing partially classified consensus sequences of *de-novo* repeat clusters. B) text file containing the online censor results. **Output:** a library containing more classified consensus sequences of *de-novo* repeat clusters (still some unknow classifications)\n",
    "\n",
    "3. **SEARCH FOR TEs IN THE GENOME ASSEMBLY WITH REPEAT-MASKER USING THE DENOVO LIB**. **Input:** genome assembly. **Output:** text file with a .out suffix containing classified TE loci with some redundancies. **Program:** [RepeatMasker](http://www.repeatmasker.org/RMDownload.html), which is a dependency of RepeatModeler.\n",
    "\n",
    "4. **ELIMINATE REDUNDANCIES IN THE REPEAT-MASKER RESULTS**. **Input:** text file with a .out suffix containing classified TE loci with some redundancies, or a directory with several .out files. **Output:** elem_stored.csv files, one per contig. Also generate other files per contig. Puts them in the same folder as the .out files used as input. **Program:** [One Code To Find Them All](http://www.biomedcentral.com/content/pdf/1759-8753-5-13.pdf).\n",
    "\n",
    "5. **INDEPENDENT SEARCH FOR LTR ELEMENTS BASED ON SECONDARY STRUCTURE**. **Input:** genome assembly, **Output:** text file with loci. **Program:** [LTRharvest](http://www.zbh.uni-hamburg.de/?id=206)\n",
    "\n",
    "6. **INDEPENDENT SEARCH FOR ELEMENTS BASED ON CODING SEQUENCES**. **Input:** genome assembly, **Output:** text file with loci. **Program:** [TransposonPSI](http://transposonpsi.sourceforge.net/)\n",
    "\n",
    "7. **ELIMINATE REDUNDANCIES AMONG PROGRAMS**\n",
    "    1. **Read OneCodeTo... results**. **Input:** Directory containing elem_stored.csv files. **Output:** Dictionary, Integer: num of elements in Dictionary.\n",
    "    \n",
    "    2. **Read LTRharvest results and eliminate redundancies chosing the longer match between the programs**. **Input:** Dictionary, Integer: num of elements in Dictionary. **Output:** Dictionary, Integer: num of elements in Dictionary.\n",
    "    \n",
    "    3. **Read TransposonPSI results and eliminate redundancies chosing the longer match between the programs**. **Input:** Dictionary, Integer: num of elements in Dictionary. **Output:** Dictionary, Integer: num of elements in Dictionary.\n",
    "    \n",
    "8. **PRINT NON-REDUNDANT OUTPUT FILE**. **Input:** Dictionary. **Output:** gff3 file.\n",
    "\n",
    "## Cookbook\n",
    "\n",
    "### Run RepeatModeler\n",
    "*RepeatModeler will produce consensus sequeces representing clusters of denovo repeat sequences, partialy classified by RepeatMasker*  \n",
    "\n",
    "<pre>\n",
    "from TE import *\n",
    "\n",
    "make_repeatmodeler_database(name='a_database',\n",
    "                            input_filename='genome_assembly_file')\n",
    "# use the BuildDatabase keyword to specify the path to your executable\n",
    "  \n",
    "run_repeatmodeler('a_database') \n",
    "# use the RepeatModeler keyword to specify the path to your executable\n",
    "</pre>\n",
    "\n",
    "The default engine is ncbi and the default lib is eukaryota. RepeatModeler should make a folder in the CWD with the file **'consensi.fa.classified'** in it. It wil write alot of temp files so don't run in Dropbox.\n",
    "\n",
    "### Run Censor online and add the classifications to your denovo library (optional)\n",
    "\n",
    "*Copy and paste the results from the webpage into a text file, here named 'Censor_results'.*\n",
    "\n",
    "<pre>\n",
    "censor_classifications = parse_online_censor('Censor_results')\n",
    "\n",
    "print_online_censor(censor_classifications,\n",
    "                    'censor_classifications_file')\n",
    "\n",
    "put_censor_classification_in_repeatmodeler_lib('consensi.fa.classified',\n",
    "                                                censor_classifications,\n",
    "                                                'consensi.fa.censor')\n",
    "\n",
    "</pre>\n",
    "\n",
    "### Run RepeatMasker using the denovo lib\n",
    "*Some contig names are too long to be parsed in RepeatMasker. However it is possible to replace the names with aliases and have the translations in a file using the first function in this section. It is important to remember to use the aliased genome assembly in the other programs as well, so that redundancies can be resolved.*\n",
    "  \n",
    "<pre>\n",
    "code_sequence_ids('genome_assembly',\n",
    "                  'names_translations',\n",
    "                  'coded_genome_assembly',\n",
    "                  'prefix_for_aliases')\n",
    "\n",
    "run_repeat_masker('coded_genome_assembly', lib = 'consensi.fa.censor', species=None) \n",
    "</pre>\n",
    "Aliases: if you give 'Mflo' as a prefix, the contig aliases will be 'Mflo_0', 'Mflo_1' ...  \n",
    "The run_repeat_masker function accepts all the RepeatMasker keywards.\n",
    "RepeatModeler will write temporary files in a new folder in the CWD so do not run in Dropbox. The output files (most importantly the .out files) will be in the same directory as the input genome assembly.\n",
    "\n",
    "\n",
    "### Ged rid of redundancies in the .out file\n",
    "*Two type of rdundancies are possible: 1) within a run, the same locus may have one classification and subsections of it may have other classifications. 2) you may want to make an additional run of RepeatMasker using the eukaryota library instead of the denovo lib. Both types are handled in this stage. You need to put the .out files of all the RepeatMasker runs in one directory and point the function to that directory.*\n",
    "\n",
    "\n",
    "<pre>\n",
    "run_OneCodeToFindThemAll('/path/to/RM/.out/files/',\n",
    "                         'name_of_intermediate_file', \n",
    "                         'octfta_output_filename', \n",
    "                         'coded_genome_assembly',\n",
    "                          build_dictionary = 'build_dictionary.pl',\n",
    "                          octfta = 'one_code_to_find_them_all.pl'\n",
    "                          )\n",
    "</pre>\n",
    "As before, the default path specified in the build_dictionary keyword is the local path on my machine\n",
    "\n",
    "\n",
    "### Run independent element searches using alternative approaches\n",
    "*LTRharvest will search for LTR secondary structures and TransposonPSI will do a blastx search against TE protein CDDs*\n",
    "<pre>\n",
    "run_LTRharvest('coded_genome_asembly', \n",
    "               'name_of_intermideate_file', \n",
    "               'ltrharvest_output')                          \n",
    "\n",
    "run_TransposonPSI('coded_genome_asembly',\n",
    "                  TPSI = 'perl transposonPSI.pl')\n",
    "</pre>\n",
    "The path for TransposonPSI is pointed to by the TPSI keyword.\n",
    "\n",
    "### Make a non-redundant data structure representing the results of all the searches\n",
    "*The data structure is a dictionary with the following structure*:\n",
    "<pre>\n",
    "TEs = { 'taken': { 'element0': {'ref': {'record': 'the line from the program's output',\n",
    "                                        'program': 'the program's name'},\n",
    "                                'contig': 'the contig's name',\n",
    "                                'start': integer,\n",
    "                                'end': integer,\n",
    "                                'length': integer,\n",
    "                                'lower_tx_level': 'element',\n",
    "                                'higher_tx_level': 'class or order or family'\n",
    "                                },\n",
    "\n",
    " \n",
    "                   'element1': {...},\n",
    "                   \n",
    "                   \n",
    "                   'element2': {...},\n",
    "                   \n",
    "                   ...\n",
    "\n",
    "\n",
    "                  },\n",
    "\n",
    "\n",
    "\n",
    "        'discarded': {...}\n",
    "\n",
    "      }\n",
    "</pre>\n",
    "The internal structure repeats itself within the 'discarded' key. The element number is unique across the taken and discarded elements.\n",
    "\n",
    "<pre>\n",
    "\n",
    "\\# puting RepeatMasker results as parsed by OneCode... in the data structure\n",
    "TEs, serial = parse_ocfa_elem_stored('/path/to/RM/.elem_stored.csv'/files/')\n",
    "\n",
    "\\# adding LTRharvest results to the data structure\n",
    "TEs, serial = integrate_ltrharvest_to_RM_TEs('ltrharvest_output',      \n",
    "                                             'coded_genome_assembly',  \n",
    "                                             serial)                      \n",
    "                                                                       \n",
    "\\# adding TransposonPSI results to the data structure                                                                 \n",
    "TEs = integrate_TransposonPSI_to_RM_TEs('NameOfInput.TPSI.allHits.chains.bestPerLocus', \n",
    "                                        'coded_genome_asembly', \n",
    "                                        TEs, \n",
    "                                        serial)\n",
    "\n",
    "</pre>\n",
    "\n",
    "Redundencies are resolved by taking the longer match across the programs  \n",
    "  \n",
    "### Make a gff3 output file\n",
    "\n",
    "<pre>\n",
    "def write_gff(TEs, 'output.gff3', max_RM_OC_score=False)\n",
    "</pre>\n",
    "\n",
    "One Code concatenates the scores of the TEs it assembles. It does not compute a composite score. By default, the concatenated scores will be written to the gff3 file, although most gff tools don't supprt that.   \n",
    "  \n",
    "However, if `max_RM_OC_score==True`, only the highset score will be retained, in which case the file will be completely complient with the schema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from Bio.Blast.Applications import NcbitblastnCommandline, NcbipsiblastCommandline\n",
    "from Bio.Blast import NCBIXML\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio import SeqFeature\n",
    "from Bio.SeqFeature import SeqFeature, FeatureLocation\n",
    "from Bio.Alphabet import IUPAC\n",
    "from Bio.Blast import NCBIXML\n",
    "import re, os, inspect, subprocess, warnings, sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReapeatMasker command line function\n",
    "==================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def code_sequence_ids(in_fasta_file_name, codes_log_filename, out_fasta_file_name, genome_code):\n",
    "          from Bio import SeqIO\n",
    "          infile = SeqIO.parse(in_fasta_file_name, 'fasta')\n",
    "          codes = open(codes_log_filename, 'wt')\n",
    "          contig_ids_coded = {}\n",
    "          coded_contigs = []\n",
    "          count = 1\n",
    "          for record in infile:\n",
    "              contig_ids_coded[genome_code+'_'+str(count)] = record.id\n",
    "              record.id = genome_code+'_'+str(count)\n",
    "              record.description = ''\n",
    "              count += 1\n",
    "              coded_contigs.append(record)\n",
    "          for code in contig_ids_coded.keys():\n",
    "              codes.write(code + '\\\\t' + contig_ids_coded[code] + '\\\\n')\n",
    "          SeqIO.write(coded_contigs, out_fasta_file_name, 'fasta')\n",
    "          codes.close()\n",
    "          return contig_ids_coded\n",
    "\n",
    "def run_repeat_masker(query, RepeatMasker = '/home/amir/homeWork/RM_package/RepeatMasker/RepeatMasker', engine=False, parallel=2,\n",
    "             slow=False, quick=False, rush=False, nolow=False, noint=False, norna=True, alu=False, div=False,\n",
    "             lib=False, cutoff=255, species='eukaryota', is_only=False, is_clip=False, no_is=False, gc=False,\n",
    "             gccalc=False, frag=60000, nocut=False, alignments=True, inv=False, lcambig=True,\n",
    "             small=False, xsmall=False, poly=False, source=False, html=False, ace=False, gff=False, u=False,\n",
    "             xm=False, no_id=True, excln=True, noisy=False):\n",
    "    \n",
    "    \n",
    "    \n",
    "    frame = inspect.currentframe()\n",
    "    args, _, _, values = inspect.getargvalues(frame)\n",
    "    del values['frame']\n",
    "                    \n",
    "    # compose the command line \n",
    "    \n",
    "    args_wo_values = ['slow','quick','rush','nolow','noint','norna','alu','is_only','is_clip','no_is'\n",
    "                      'gccalc','nocut','nocut','alignments','inv','lcambig','small','xsmall','poly',\n",
    "                      'source','html','ace','gff','u','xm','no_id','excln','noisy']\n",
    "    \n",
    "    cline = RepeatMasker + ' '\n",
    "    for arg in values.keys():\n",
    "        if not arg in ['RepeatMasker','query']:\n",
    "            if not arg in args_wo_values and not values[arg] == False:\n",
    "                cline = cline + '-' + arg + ' ' + str(values[arg]) + ' '\n",
    "            elif values[arg] == True:\n",
    "                cline = cline + '-' + arg + ' '\n",
    "\n",
    "    cline = cline + query\n",
    "    \n",
    "    # execute the command\n",
    "    print cline\n",
    "    return os.system(cline), query + '.out' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RepeatModeler functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_repeatmodeler_database(name, input_filename, BuildDatabase='/home/amir/homeWork/RM_package/RepeatModeler/BuildDatabase',\n",
    "        dir=False, engine='ncbi', batch=False):\n",
    "    \n",
    "    frame = inspect.currentframe()\n",
    "    args, _, _, values = inspect.getargvalues(frame)\n",
    "    del values['frame']\n",
    "       \n",
    "    # compose the command line        \n",
    "    cline = BuildDatabase + ' '\n",
    "    for arg in values.keys():\n",
    "        if not arg in ['BuildDatabase']:\n",
    "            if not values[arg] == False and not values[arg] == True and not arg == 'input_filename':\n",
    "                cline = cline + '-' + arg + ' ' + str(values[arg]) + ' '\n",
    "    cline = cline + input_filename\n",
    "    \n",
    "    # execute the command\n",
    "    print cline\n",
    "    return os.system(cline)\n",
    "\n",
    "def run_repeatmodeler(database, RepeatModeler='perl /home/amir/homeWork/RM_package/RepeatModeler/RepeatModeler', engine='ncbi',\n",
    "                     species='eukaryota'):\n",
    "    frame = inspect.currentframe()\n",
    "    args, _, _, values = inspect.getargvalues(frame)\n",
    "    del values['frame']\n",
    "    \n",
    "    cline = (RepeatModeler + ' -engine ' + engine + ' -database ' + database +\n",
    "             ' > ' + database + '.out')\n",
    "    print cline\n",
    "    return os.system(cline)\n",
    "\n",
    "def find_result_dirs_per_genome_code():\n",
    "    \n",
    "    \"\"\"CWD should be a direcrtory containing a set of subdirs with\n",
    "    RepeatModeler results. This returns a dict with the genome codes\n",
    "    as keys and the dir name as values\"\"\"\n",
    "    \n",
    "    RM_results_subdirectories = {}\n",
    "    for sub in os.walk('.').next()[1]:\n",
    "        if sub[0:2] == 'RM':\n",
    "            folder_name = sub\n",
    "            Genome_code = open('./'+sub+'/round-1/sampleDB-1.fa','r').readlines()[0].split()[1].split('-')[0]\n",
    "            RM_results_subdirectories[Genome_code]=folder_name\n",
    "    return RM_results_subdirectories\n",
    "#print find_results_dirs_per_genome_code()\n",
    "\n",
    "# {'Gros': 'RM_6449.WedMay210843002014_Gros'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Parse online CENSOR results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_online_censor(filename, pident_cutoff, score_cutoff):\n",
    "    censor_classifications = {}\n",
    "    keep_parsing = True\n",
    "    lines = open(filename,'r').readlines()\n",
    "    for line in lines:\n",
    "        if \"Masked Sequence\" in line:\n",
    "            keep_parsing = False\n",
    "        parse_this_line = True\n",
    "        NOT = ('[GIRI]', 'Home ', 'Map of Hits', 'SVG', 'Name ')\n",
    "        for n in NOT:\n",
    "            if n in line or line[0]=='\\t' or line[0]==' ' or line[0]=='\\n':\n",
    "                parse_this_line = False\n",
    "        if keep_parsing and parse_this_line:\n",
    "            l = line.rstrip()\n",
    "            name = l.split('\\t')[0][:-1]\n",
    "            Class = l.split('\\t')[6][:-1]\n",
    "            score = int(l.split('\\t')[-1])\n",
    "            pident = float(l.split('\\t')[-3])\n",
    "            if (name in censor_classifications.keys() and\n",
    "                score >= score_cutoff and pident >= pident_cutoff):\n",
    "                if score > censor_classifications[name]['score']:\n",
    "                    censor_classifications[name]['score'] = score\n",
    "                    censor_classifications[name]['Class'] = Class\n",
    "                    censor_classifications[name]['pident'] = pident\n",
    "            elif score >= score_cutoff and pident >= pident_cutoff:\n",
    "                censor_classifications[name] = {'score': score,\n",
    "                                                'Class': Class,\n",
    "                                                'pident': pident}\n",
    "    return censor_classifications\n",
    "            \n",
    "def print_online_censor(censor_classifications, filename):\n",
    "    import csv\n",
    "    with open(filename, 'wb') as csvfile:\n",
    "        linewriter = csv.writer(csvfile, delimiter='\\t',\n",
    "                                quotechar='|',\n",
    "                                quoting=csv.QUOTE_MINIMAL) \n",
    "        linewriter.writerow(['Name','calss','score'])\n",
    "        for name in censor_classifications.keys():\n",
    "            line = [name, censor_classifications[name]['Class'], censor_classifications[name]['score']]\n",
    "            linewriter.writerow(line)\n",
    "            \n",
    "def put_censor_classification_in_repeatmodeler_lib(input_filename, censor_classifications, output_filename):\n",
    "    from Bio import SeqIO\n",
    "    RM_lib = SeqIO.parse(input_filename, 'fasta')\n",
    "    RM_CENCOR_lib = []\n",
    "    for record in RM_lib:\n",
    "        if 'Unknown' in record.id and record.id in censor_classifications.keys():\n",
    "            classification = censor_classifications[record.id]['Class']\n",
    "            record.id = record.id.split('#')[0]+'#'+classification\n",
    "            record.description = (' ').join(record.description.split(' ')[1:])\n",
    "            RM_CENCOR_lib.append(record)\n",
    "        else:\n",
    "            RM_CENCOR_lib.append(record)\n",
    "    SeqIO.write(RM_CENCOR_lib,output_filename,'fasta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Code To Find Them All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_OneCodeToFindThemAll(pooled_RM_outputs_dir, #The directory containing repeatmasker .out files or a path to a specific .out file\n",
    "                            ltr_dict_filename, # name of intermediate file\n",
    "                            output_filename, \n",
    "                            genome_assembly,\n",
    "                            unknown=False,\n",
    "                            strict=True,\n",
    "                            build_dictionary = '/home/amir/homeWork/RM_package/OCtFtA/build_dictionary.pl',\n",
    "                            octfta = '/home/amir/homeWork/RM_package/OCtFtA/one_code_to_find_them_all.pl',\n",
    "                            ):\n",
    "    cline = build_dictionary+' --rm '+pooled_RM_outputs_dir+' --unknown > '+ ltr_dict_filename\n",
    "    os.system(cline)\n",
    "    \n",
    "    cline = octfta+' --rm '+pooled_RM_outputs_dir+' --ltr '+ltr_dict_filename+' --fasta '+genome_assembly\n",
    "    if unknown:\n",
    "        cline += ' --unknown'\n",
    "    if strict:\n",
    "        cline += ' --strict'\n",
    "    cline +=' > '+output_filename\n",
    "    os.system(cline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LTRharvest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_LTRharvest(input_filename, index_name, output_name):\n",
    "    cline = 'gt suffixerator -db '+input_filename+' -indexname '+index_name+' -tis -suf -lcp -des -ssp -sds -dna'\n",
    "    os.system(cline)\n",
    "    cline = 'gt ltrharvest -index '+index_name+' -mintsd 5 -maxtsd 100 > '+output_name\n",
    "    os.system(cline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TransposonPSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_TransposonPSI(input_filename,\n",
    "                  TPSI = 'perl /home/amir/homeWork/RM_package/TransposonPSI_08222010/transposonPSI.pl'):\n",
    "    cline = (TPSI+' '+input_filename+' nuc')\n",
    "    os.system(cline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unite OCTFTA with LTRharvest and TransposonPSI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse OCTFTA elem_stored.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_ocfa_elem_stored(pooled_RM_outputs_dir):\n",
    "\n",
    "    # Get all the '.elem_stored.csv' file names\n",
    "    from glob import glob\n",
    "    filenames = glob(pooled_RM_outputs_dir + '*.elem_sorted.csv')\n",
    "\n",
    "    # An empty dict to hold final TE list\n",
    "    TEs = {'taken': {}, 'discarded': {}}\n",
    "    serial = 1\n",
    "\n",
    "    # Get all the elements in the OCTFTA output\n",
    "    for filename in filenames:\n",
    "        taken_elements = {}\n",
    "        discarded_elements = {}\n",
    "        for line in open(filename, 'r').readlines():\n",
    "            if line[:3] == '###':\n",
    "                \n",
    "                reference = {'program': 'RMOCFA',\n",
    "                             'record': line}\n",
    "                contig = line.split('\\t')[4]\n",
    "                start = line.split('\\t')[5]\n",
    "                end = line.split('\\t')[6]\n",
    "                length = line.split('\\t')[7]\n",
    "                element = line.split('\\t')[9]\n",
    "                family = line.split('\\t')[10]\n",
    "                \n",
    "                max_score = max([int(i) for i in line.split('\\t')[0][3:].split('/')])\n",
    "                \n",
    "                take = True\n",
    "                \n",
    "                # make sure the locus is not covered and if it is check which match is better\n",
    "                for element in taken_elements:\n",
    "                    prex_el_line = taken_elements[element]['ref']['record']\n",
    "                    prex_el_score = max([int(i) for i in prex_el_line.split('\\t')[0][3:].split('/')])\n",
    "                    prex_el_contig = taken_elements[element]['contig']\n",
    "                    prex_el_start = taken_elements[element]['start']\n",
    "                    prex_el_end = taken_elements[element]['end']\n",
    "                    if (contig == prex_el_contig and \n",
    "                        (prex_el_start < start < prex_el_end or\n",
    "                         prex_el_start < end < prex_el_end or\n",
    "                         start < prex_el_start < end or\n",
    "                         start < prex_el_end <end)):\n",
    "                        if max_score > prex_el_score:\n",
    "                            discarded_elements[element] = taken_elements.pop(element, None)\n",
    "                        else:\n",
    "                            take = False\n",
    "                        \n",
    "                if take:\n",
    "                    taken_elements['element' + str(serial)] = {'ref': reference,\n",
    "                                                              'contig': contig,\n",
    "                                                              'start': int(start),\n",
    "                                                              'end': int(end),\n",
    "                                                              'length': int(length),\n",
    "                                                              'lower_tx_level': element,\n",
    "                                                              'higher_tx_level': family}\n",
    "\n",
    "                    \n",
    "                serial += 1\n",
    "        TEs['taken'].update(taken_elements)\n",
    "        TEs['discarded'].update(discarded_elements)\n",
    "    return TEs, serial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get loci from the LTRharvest output only if they are longer than ones found with repeatmasker for the same locus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def integrate_ltrharvest_to_RM_TEs(LTRharvest_output_filename,genome_path, TEs_from_RMOCFA, serial, sim_cutoff=85, l_cutoff=4000 ):\n",
    "    import re\n",
    "    from Bio import SeqIO\n",
    "    \n",
    "    lines = open(LTRharvest_output_filename, 'r').readlines()\n",
    "    \n",
    "    contig_names = [record.id for record in SeqIO.parse(genome_path, 'fasta')]\n",
    "    \n",
    "    line_count = 0\n",
    "    for line in lines:\n",
    "        if not line[0] == '#' and len(line) > 1:\n",
    "            # correct contig name:\n",
    "            ## get the true contig name based on the sequence number in the LTRarvest output:\n",
    "            from Bio import SeqIO\n",
    "            corrected_sequence_name = None\n",
    "            line_serial = int(line.rstrip().split('  ')[-1])\n",
    "            \n",
    "            try: \n",
    "                corrected_sequence_name = contig_names[line_serial]\n",
    "            except:\n",
    "                raise RuntimeError('Could not find contig for seq number ' + str(line_serial))\n",
    "                \n",
    "            ## Parse the LTRharvest results line\n",
    "            reference = {'program': 'LTRharvest',\n",
    "                         'record': line}\n",
    "            contig = corrected_sequence_name\n",
    "            start = int(line.split('  ')[0])\n",
    "            end = int(line.split('  ')[1])\n",
    "            length = int(line.split('  ')[2])\n",
    "            lower_tx_level = '?'\n",
    "            higher_tx_level = 'LTR'\n",
    "            sim = float(line.split('  ')[-2])\n",
    "            l = int(line.split('  ')[2])\n",
    "            TE = {'ref': reference,\n",
    "                  'contig': contig,\n",
    "                  'start': int(start),\n",
    "                  'end': int(end),\n",
    "                  'length': int(length),\n",
    "                  'lower_tx_level': lower_tx_level,\n",
    "                  'higher_tx_level': higher_tx_level}\n",
    "            \n",
    "            ## Check if the locus is already covered by the repeatmasker results\n",
    "            ## If it is, check if the ltr hit is longer (then place in taken, and move the rm hit to discarded)\n",
    "            ## or shorter (then place the ltr hit in discraded)\n",
    "            placed = False\n",
    "            for key in TEs_from_RMOCFA['taken'].keys():\n",
    "                if ( TEs_from_RMOCFA['taken'][key]['contig'] == contig and \n",
    "                    (TEs_from_RMOCFA['taken'][key]['start']< start <TEs_from_RMOCFA['taken'][key]['end'] or \n",
    "                    TEs_from_RMOCFA['taken'][key]['start']< end <TEs_from_RMOCFA['taken'][key]['end'] or\n",
    "                    start < TEs_from_RMOCFA['taken'][key]['start'] < end or\n",
    "                    start < TEs_from_RMOCFA['taken'][key]['end']  < end)):\n",
    "                    ### since it is, keep the longer output (either repeatmasker or LTRharvest)\n",
    "                    ### use the repeatmasker classification either way\n",
    "                    ### put the looser in the 'discarded' dictionary\n",
    "                    if TEs_from_RMOCFA['taken'][key]['length'] < length and sim >= sim_cutoff and l >= l_cutoff:\n",
    "                        #TE['element'] = TEs_from_RMOCFA['taken'][key]['lower_tx_level']\n",
    "                        #TE['family'] = TEs_from_RMOCFA['taken'][key]['higher_tx_level']\n",
    "                        TEs_from_RMOCFA['discarded'][key] = TEs_from_RMOCFA['taken'].pop(key, None)\n",
    "                        TEs_from_RMOCFA['taken']['element'+str(serial)] = TE\n",
    "                    else:\n",
    "                        TEs_from_RMOCFA['discarded']['element'+str(serial)] = TE\n",
    "                    placed = True\n",
    "                    break\n",
    "            if not placed and sim >= sim_cutoff  and l >= l_cutoff:\n",
    "                ### Since it is not, add the LTRharvest TE to the 'taken' dict:\n",
    "                TEs_from_RMOCFA['taken']['element'+str(serial)] = TE\n",
    "                serial +=1\n",
    "            else:\n",
    "                TEs_from_RMOCFA['discarded']['element'+str(serial)] = TE\n",
    "            serial +=1    \n",
    "            #if line_count%100 == 0:\n",
    "            #    print str(line_count)\n",
    "            line_count += 1\n",
    "    return TEs_from_RMOCFA, serial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get loci from the TransposonPSI output only if they are longer than ones found with repeatmasker for the same locus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def integrate_TransposonPSI_to_RM_TEs(TransposonPSI_output_filename,genome_path, TEs_from_RMOCFA, serial, score_cutoff=100):\n",
    "    import re\n",
    "    \n",
    "    lines = open(TransposonPSI_output_filename, 'r').readlines()\n",
    "    \n",
    "    line_count = 0\n",
    "    for line in lines:\n",
    "        if line[0] == '#':\n",
    "                \n",
    "            ## Parse the TransposonPSI results line\n",
    "            reference = {'program': 'TransposonPSI',\n",
    "                         'record': line}\n",
    "            contig = line.split('\\t')[3]\n",
    "            start = int(line.split('\\t')[4].split('-')[0])\n",
    "            end = int(line.split('\\t')[4].split('-')[1])\n",
    "            length = end-start+1\n",
    "            lower_tx_level = '?'\n",
    "            higher_tx_level = line.split('\\t')[1]\n",
    "            score = line.split('\\t')[-1].rstrip()\n",
    "            TE = {'ref': reference,\n",
    "                  'contig': contig,\n",
    "                  'start': int(start),\n",
    "                  'end': int(end),\n",
    "                  'length': int(length),\n",
    "                  'lower_tx_level': lower_tx_level,\n",
    "                  'higher_tx_level': higher_tx_level}\n",
    "            \n",
    "            ## Check if the locus is already covered by previous results\n",
    "            placed = False\n",
    "            for key in TEs_from_RMOCFA['taken'].keys():\n",
    "                if ( TEs_from_RMOCFA['taken'][key]['contig'] == contig and \n",
    "                    (TEs_from_RMOCFA['taken'][key]['start']< start <TEs_from_RMOCFA['taken'][key]['end'] or \n",
    "                    TEs_from_RMOCFA['taken'][key]['start']< end <TEs_from_RMOCFA['taken'][key]['end']or\n",
    "                    start < TEs_from_RMOCFA['taken'][key]['start'] < end or\n",
    "                    start < TEs_from_RMOCFA['taken'][key]['end']  < end)):\n",
    "                    ### since it is, keep the longer output \n",
    "                    ### put the looser in the 'discarded' dictionary\n",
    "                    if TEs_from_RMOCFA['taken'][key]['length'] < length and score >= score_cutoff:\n",
    "                        TEs_from_RMOCFA['discarded'][key] = TEs_from_RMOCFA['taken'].pop(key, None)\n",
    "                        TEs_from_RMOCFA['taken']['element'+str(serial)] = TE\n",
    "                    else:\n",
    "                        TEs_from_RMOCFA['discarded']['element'+str(serial)] = TE\n",
    "                    placed = True\n",
    "                    break\n",
    "            if not placed and score >= score_cutoff:\n",
    "                ### Since it is not, add the TransposonPSI TE to the 'taken' dict:\n",
    "                TEs_from_RMOCFA['taken']['element'+str(serial)] = TE\n",
    "            else:\n",
    "                TEs_from_RMOCFA['discarded']['element'+str(serial)] = TE\n",
    "            serial +=1\n",
    "            #if line_count%100 == 0:\n",
    "            #    print str(line_count)\n",
    "            line_count += 1\n",
    "    return TEs_from_RMOCFA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TE dict to gff3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_gff(TEs, gff_filename, max_RM_OC_score=False): \n",
    "\n",
    "    gff_pattern = \"%s\\t%s\\ttransposable_element\\t%i\\t%i\\t%s\\t%s\\t.\\tID=%s;Name=%s;Note=%s\\n\" \n",
    "    #%(contig, program, start, end, score, strand, ID, name, note)\n",
    "\n",
    "    \n",
    "    # Make the regions bit for the top of the file\n",
    "    regions = {}\n",
    "\n",
    "    for e in TEs['taken']:\n",
    "        record = TEs['taken'][e]\n",
    "        contig = record['contig']\n",
    "        start, end = record['start'], record['end']\n",
    "        if start > end:\n",
    "            start, end = end, start\n",
    "            \n",
    "        # Each contig has to be included once and encopass all the TEs\n",
    "        # that are on it\n",
    "        if not contig in regions:\n",
    "            regions[contig] = [start,end]\n",
    "        else:\n",
    "            if start < regions[contig][0]:\n",
    "                regions[contig][0] = start\n",
    "            if end > regions[contig][1]:\n",
    "                regions[contig][1] = end\n",
    "\n",
    "    regions = sorted(regions.items(), key = lambda i: i[0])\n",
    "    regions = [\"##sequence-region   %s %i %i\\n\"%(j[0],j[1][0],j[1][1]) for j in regions]\n",
    "\n",
    "    \n",
    "    # Write the file\n",
    "    with open(gff_filename,'wt') as gff:\n",
    "        # Write the regions\n",
    "        gff.write('##gff-version 3\\n')\n",
    "        for l in regions:\n",
    "            gff.write(l)\n",
    "        # Write the matches\n",
    "        for e in TEs['taken']:\n",
    "            record = TEs['taken'][e]\n",
    "            contig = record['contig']\n",
    "            program = record['ref']['program']\n",
    "            if program == 'RMOCFA':\n",
    "                program = 'Repeatmasker-OneCode'\n",
    "            start, end = record['start'], record['end']\n",
    "            # make sure start is the smaller coordinate\n",
    "            if start > end:\n",
    "                start, end = end, start\n",
    "            ID = e\n",
    "            name = record['higher_tx_level']\n",
    "            note = record['lower_tx_level']\n",
    "            if 'element' in note:\n",
    "                note = '?'\n",
    "            score, strand = '.', '.'\n",
    "            ref = record['ref']['record'].rstrip()\n",
    "            if program == 'TransposonPSI':\n",
    "                score, strand = ref.split('\\t')[-1], ref.split('\\t')[-2]\n",
    "            elif program == 'Repeatmasker-OneCode':\n",
    "                score, strand = ref.split('\\t')[0], ref.split('\\t')[8]\n",
    "                if max_RM_OC_score:\n",
    "                    score = max([int(s) for s in score.split('/')])\n",
    "            strand = strand.replace('C','-')\n",
    "            score = score.replace('#','')\n",
    "            gff.write(gff_pattern%(contig, program, start, end, score, strand, ID, name, note))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils for multiple genome assembly analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def genome_codes_list(genomes_directory, mode='++', code_file = 'genome_assembly_files.csv'):\n",
    "    \"\"\" return a list of codes \n",
    "    genomes_directory: path to the directory containing the genome assemblies\n",
    "    code file: contains code and genome assembly file names (wo path).\n",
    "    It is formated as follows:\n",
    "    \n",
    "    <code><space><assembly filename><space><mode><newline>\n",
    "    \n",
    "    mode is any symbol grouping the filenames. It is nested. \n",
    "    examples:\n",
    "    \n",
    "    mode = '$'\n",
    "    \n",
    "    code1 filename1 $ -> will be read\n",
    "    code2 filename2 $$ -> will be read\n",
    "    code3 filename3 $+ -> will be read\n",
    "    code4 filename4 + -> will not be read\n",
    "    \n",
    "    mode = '$$'\n",
    "    \n",
    "    with the same example as above, only code2 will be read.\n",
    "    \n",
    "    \"\"\"\n",
    "    codes = []\n",
    "    for line in open(genomes_directory+code_file,'r').readlines():\n",
    "        if mode in line:\n",
    "            codes.append(line.split()[0])\n",
    "    return codes\n",
    "\n",
    "def genomes_dict(genomes_directory, mode='++', code_file = 'genome_assembly_files.csv'):\n",
    "    \"\"\" returns a dict, codes as keys, file names as values \"\"\"\n",
    "    genomes = {}\n",
    "    for line in open(genomes_directory+code_file,'r').readlines():\n",
    "        if mode in line:\n",
    "            genomes[line.split()[0]] = line.split()[1]\n",
    "    return genomes\n",
    "\n",
    "def assembly_of(code, genomes_directory, generator=True, code_file = 'genome_assembly_files.csv', mode='++'):\n",
    "    \"\"\" returns a list of SeqRecords \n",
    "    which are the contig of the assembly.\n",
    "    \"\"\"\n",
    "    from Bio import SeqIO\n",
    "    records = SeqIO.parse(genomes_directory+genomes_dict(genomes_directory=genomes_directory, code_file=code_file, mode='++')[code],'fasta')\n",
    "    if not generator:\n",
    "        records = list(records)\n",
    "    return records\n",
    "\n",
    "\n",
    "def codes_with_no_censor_lib(folders):\n",
    "    \"\"\" return codes with no denovo lib \"\"\"\n",
    "    import os\n",
    "    missing_censor_results = []\n",
    "    for path in codes_with_folders(folders=folders):\n",
    "        pass\n",
    "        if not os.path.isfile(path+'consensi.fa.censor'):\n",
    "            missing_censor_results.append(path.split('/')[-2])\n",
    "    return missing_censor_results\n",
    "\n",
    "def codes_with_censor_lib(folders):\n",
    "    \"\"\" return codes that have a censor denovo lib \"\"\"\n",
    "    import os\n",
    "    missing_censor_results = []\n",
    "    for path in codes_with_folders(folders=folders):\n",
    "        pass\n",
    "        if os.path.isfile(path+'consensi.fa.censor'):\n",
    "            missing_censor_results.append(path.split('/')[-2])\n",
    "    return missing_censor_results\n",
    "\n",
    "def make_non_redundant_lib(redundant_lib, non_redundant_lib,\n",
    "                           cluster_identity=0.9, uclust = 'uclust'):\n",
    "    \"\"\" makes a non redundant lib \"\"\"\n",
    "    \n",
    "    import os\n",
    "    cline = uclust+' --sort '+redundant_lib+' --output seqs_sorted.fasta'\n",
    "    os.system(cline)\n",
    "    cline = uclust+' --input seqs_sorted.fasta --uc results.uc --id '+str(cluster_identity)\n",
    "    os.system(cline)\n",
    "    cline = uclust+' --uc2fasta results.uc --input seqs_sorted.fasta  --types S --output results.fasta'\n",
    "    os.system(cline)\n",
    "    from Bio import SeqIO\n",
    "    from Bio.Seq import Seq\n",
    "    records = list(SeqIO.parse('results.fasta', 'fasta'))\n",
    "    for r in records:\n",
    "        r.id = r.id.split('|')[2]\n",
    "        r.description = ' '.join(r.description.split()[1:])\n",
    "        alpha = r.seq.alphabet\n",
    "        r.seq = Seq(str(r.seq).replace('-',''), alphabet=alpha)\n",
    "        \n",
    "    SeqIO.write(records,non_redundant_lib,'fasta')\n",
    "    return non_redundant_lib\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
